---
title: "ML_biomarker"
author: "Emily Ricotta"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(broom)
library(tidyverse)
library(data.table)
library(magrittr)
library(Hmisc)
library(here)
library(funModeling)
library(conflicted)
library(knitr)
library(skimr)
library(ggplot2)
library(GGally)
library(AER)
library(BBmisc)
library(tidymodels)
library(tidyclust)
library(embed)
library(dials)

options(tibble.print_max = Inf, tibble.width = Inf, digits=7, scipen=999, max.print=5000)
knitr::opts_chunk$set(echo = FALSE)

conflict_prefer("describe", "Hmisc")
conflict_prefer("%nin%", "Hmisc")
conflict_prefer("select", "dplyr")
conflict_prefer("filter", "dplyr")


here() 
```


```{r load data, include=FALSE}
#From Serology_reading cleaning ELISA plates.R:
df_dat1 <- file.info(list.files(paste0(here(), "/Data"), full.names = T))
max_dat1<-df_dat1[grepl("dat1", rownames(df_dat1)),]
max_dat1<-rownames(max_dat1)[which.max(max_dat1$mtime)]
load(max_dat1)

df_dems <- file.info(list.files(paste0(here(), "/Data"), full.names = T))
max_dems<-df_dems[grepl("dems", rownames(df_dems)),]
max_dems<-rownames(max_dems)[which.max(max_dems$mtime)]
load(max_dems)

```

```{r normalize the data}

dat1 %<>% 
  mutate(value_normalize = normalize(value_tobit,
                                     method = "standardize"))


dat1_wide<-pivot_wider(dat1 %>% 
                         arrange(Analyte),
                       id_cols = c("VIALID"),
                       names_from = "Analyte",
                       values_from = "value_normalize")

dat1_wide<-left_join(dems %>% 
                       select(c("VIALID", "pat_type", "age_c", "died", 
                                "Sex", "severity", "any_covid")),
                     dat1_wide,
                     by = "VIALID")

dat_tidy<-dat1_wide %>%
  na.omit()

```

```{r t-sne}
#https://github.com/jkrijthe/Rtsne
library(Rtsne)
set.seed(410)

tsne_out <- Rtsne(as.matrix(dat_tidy[,c(8:66)]))
plot(tsne_out$Y,
     col = factor(dat_tidy$pat_type),
     asp=1) 

tout<-data.frame(X = tsne_out$Y[,1],
                 Y = tsne_out$Y[,2])
tout<-bind_cols(tout, 
                dat_tidy)
tout %<>% 
  arrange(X)
low<-tout %>% 
  filter(X < -10 & Y < -9)
rest<-tout %>% 
  filter(X > -10 & Y > -9)

t_only<-dat_tidy %>% 
  filter(pat_type == "trauma")
tsne_trauma<-Rtsne(as.matrix(t_only[,c(8:66)]))
plot(tsne_trauma$Y,
     col = factor(t_only$pat_type),
     asp=1) 

tout<-data.frame(X = tsne_trauma$Y[,1],
                 Y = tsne_trauma$Y[,2])
tout<-bind_cols(tout, 
                t_only)
tout %<>% 
  (X)
low<-tout %>% 
  filter((X > 0 & 
            X < 20) & 
           (Y > 15))
rest<-tout %>% 
  filter(VIALID %nin% low$VIALID)

```

```{r Random Forest trauma}
library(tidymodels)
library(caret)
library(ranger)
library(e1071)
library(vip)

conflicted::conflict_prefer("slice", "dplyr")


dat_1h<-dat_tidy %>% 
  mutate(pat_type = case_when(pat_type == "trauma" ~ 1,
                              TRUE ~ 0),
         pat_type = as.factor(pat_type),
         Sex = case_when(Sex == "Female" ~ 0,
                         TRUE ~ 1)) %>% 
  select(-c("severity", "any_covid", "died")) 

#parameters
#params=list(seed=seed number, 
#            prop=proportion for splitting testing and training data, 
#            IDVars=list of ID variables to reassign later,
#            nCores= number physical CPUs,
#            nThreads=number physical and theoretical CPUs),
#            ntrees=number of trees in model,
#            ngrid=size of grid for grid search in tuning,
#            ncv=number of times model was split for cross-validation
#            min_n=min number of data points needed for node to split; set to 
#              FALSE initially for tuning
#            mtry=min number of variables considered for each split in tree; set
#               to FALSE initially for tuning

# set original parameters for this analysis
params_list = list(seed=110,
                   prop=0.66,
                   IDVars=c("VIALID"),
                   ntrees=10, 
                   ngrid=20, 
                   ncv=5,
                   min_n=FALSE,
                   mtry=FALSE)

# Function for getting training and testing models
get_all_models = function(df, params){
  
  # structuring recipe
  rec = cookbook(df)
  
  #get splits, train/test data
  #d_list output is 1) data splits 2) training data 3) test data
  d_list = splitz(df, params)
  
  #tune random forest hyperparams with grid_search from training data
  m_spec = tune_import_og(params) #specify tuning model 
  t_model = tune_folds(d_list[[2]], m_spec, rec, params) #run grid search
  t_metrics = t_model %>% 
    collect_metrics() #pull metrics from grid search
  t_metrics$trees = params$ntrees #pull ntrees from params
  
  #select model with highest roc_auc
  f_params = select_best(t_model, "roc_auc")
  f_model = finalize_mod(f_params, params)
  
  # obtain the model output for the test set
  v_model = valid_mod(d_list[[1]], rec, f_model)
  v_metrics = v_model %>% 
    collect_metrics() %>% 
    cbind(f_params)
  v_metrics$trees = params$ntrees
  
  # capture all model output in a list and return
  list('train_metrics' = t_metrics,
       'train_notes' = t_model$.notes,
       'best_model' = f_model, 
       'test_metrics' = v_metrics)
}

#Prep data function
juicer = function(df){
  cookbook(df) %>%
    juice()
}

####
# make recipe
####
#make recipe w/ model and dataset; specify that ids aren't predictors
#rec is recipe name for other functs. (syntax rec=cookbook(args))
#prep() to update recipe with parameters that don't need data for training
cookbook = function(df){
  recipe(formula = pat_type ~ .,
         data = df) %>%
    update_role(c(VIALID),
                new_role = "ID") #%>%
    #prep()
}


#Split data function
splitz = function(df, params){
  
  #set seed for reproducibility
  set.seed(params$seed)
  
  #make split object
  data_split = initial_split(df,
                             prop = params$prop,
                             strata = "pat_type")
  #create train and test data
  train = training(data_split)
  test = testing(data_split)
  
  list(data_split, train, test)
}


#function for initial tuning model to plug into get_all_models
tune_import_og = function(params){
  
  #make tuning model
  a_model = rand_forest(
    mtry = tune(),
    trees = !!params$ntrees,
    min_n = tune() 
  ) 
  
  a_model %>%
    set_mode("classification") %>%
    set_engine("ranger", 
               seed = !!params$seed)
}                 

#new function for tuning to plug into other functions to be used after og tuning
#
#different from other tuning function because it collects permutation importance;
#   this isn't collected with initial tuning because it's computationally 
#   expensive and only necessary  for final model
tune_import_new = function(params){
  
  #make tuning model
  a_model = rand_forest(
    mtry = !!params$mtry,
    trees = !!params$ntrees,
    min_n = !!params$min_n 
  ) 
  
  
  a_model %>% 
    set_mode("classification") %>%
    set_engine("ranger", 
               importance = "permutation",
               seed = !!params$seed)
}

####
# tune model hyperparameters plus cross validation
####
tune_folds = function(df, model, rec, params){
  tunewf = workflow() %>%
    add_model(model) %>% 
    add_recipe(rec)
  
  set.seed(params$seed)
  folds = vfold_cv(df,
                   params$ncv)
  
  #if ngrid is a number use grid search to find best hyperparams
  #after tuning, change ngrid to FALSE to trigger else and cross validate hyperparams
  
  if(params$ngrid){
    set.seed(params$seed)
    tune_grid(
      tunewf,
      resamples = folds,
      grid = params$ngrid,
      metrics = metric_set(pr_auc, roc_auc)
    )
  } else{
    control = control_resamples(save_pred = TRUE)
    set.seed(params$seed)
    fit_resamples(tunewf,
                  folds,
                  metrics = metric_set(pr_auc, roc_auc),
                  control = control)
  }
  
}

#tunes model with old parameters, then updates with new ones
finalize_mod <- function(new_parms, params){
  tune_spec <- tune_import_new(params)
  
  finalize_model(
    tune_spec,
    new_parms
  )
}

#validates model 
#fits data one more time on training data, then validates on test set
#used in fin_mod_import
valid_mod <- function(data_splits, rec, fin_rf){
  final_wf <- workflow() %>%
    add_recipe(rec) %>%
    add_model(fin_rf)
  
  final_wf %>%
    last_fit(data_splits,
             metrics = metric_set(pr_auc, roc_auc))
}


#extracts variable importance after tuning model
#used in fin_mod_import
v_important = function(df, final_mod, rec, params){
  import_mod = final_mod
  
  if(pluck(params, 'ngrid')){
    import_mod %<>% 
      set_engine("ranger", 
                 importance = "permutation",
                 seed = !!params$seed)
  }
  
  import_wf = workflow() %>%
    add_model(import_mod) %>% 
    add_recipe(rec)
  
  imp_fit = import_wf %>% 
    fit(df)
  
  pluck(imp_fit, 'fit', 'fit', 'fit', 'variable.importance') %>% 
    sort(decreasing = TRUE)
}


#trains and validates  final model and extracts variable importance
#output is a list:
#  list('train_model' = t_model,
#   'train_metrics' = t_metrics, 
#   'train_predictions' = train_pred_df,
#   'train_variable_importances' = train_var_imps,
#   'test_model' = val_m,
#   'test_metrics' = val_metrics,
#   'test_predictions' = test_pred_df,
#   'test_variable_importances' = test_var_imps)


finmod_import = function(df, params){
  # structuring recipe
  rec = cookbook(df)
  
  # getting splits and dataframes for train/test data
  # d_list output is 1) data splits 2) training data 3) test data
  d_list = splitz(df,params)
  
  # set a RF model specification with designated variables from grid search
  m_spec = tune_import_new(params)
  
  # train model given a recipe and model parameters
  print("Training model...")
  t_model = tune_folds(d_list[[2]], m_spec, rec, params)
  t_metrics = t_model %>% 
    collect_metrics()
  t_metrics$trees = params$ntrees
  
  #map train predictions to patient id
  print("Mapping training predictions...")
  train_pred_df = map(t_model$.predictions,
                      function(x){
                        xRows = x$.row
                        vialid = d_list[[2]] %>% 
                          slice(xRows) %>% 
                          pull(VIALID)
                        cbind(x, "VIALID" = vialid)
                      }) %>% 
    rbindlist(idcol = "cv_split")
  
  train_var_imps = v_important(d_list[[2]], m_spec, rec, params)
  test_var_imps = v_important(d_list[[3]], m_spec, rec, params)
  
  # run the model on the test set
  rf_params = params[c('mtry', 'min_n', 'ntrees')] %>% data.frame()
  
  print("Validating model...")
  val_m = valid_mod(d_list[[1]], rec, m_spec)
  val_metrics = cbind(rf_params, val_m %>% collect_metrics())
  
  #map patient ids to test predictions
  print("Mapping test predictions...")
  test_pred_df = map_dfr(val_m$.predictions, 
                         function(x){
                           vialid = d_list[[3]] %>%  
                             pull(VIALID)
                           cbind(x, "VIALID" = vialid)
                         }) %>% 
    as_tibble()
  
  
  # capture all model output in a list and return
  list('train_model' = t_model,
       'train_metrics' = t_metrics, 
       'train_predictions' = train_pred_df,
       'train_variable_importances' = train_var_imps,
       'test_model' = val_m,
       'test_metrics' = val_metrics,
       'test_predictions' = test_pred_df,
       'test_variable_importances' = test_var_imps)
  
}


#get best parameters from the sample splits
#not used if testing whole dataset -- only for multiple samples/subsets of data
obtain_parameters <- function(L_out){
  controlEvals = map_dfr(1:length(L_out), 
                         function(x) pluck(L_out, x, 'test_metrics')) %>% 
    mutate(uniq_param1 = paste0(mtry, '-', min_n),
           uniq_param2 = paste0(mtry, '-', min_n, '-', .metric))
  
  countEvals = controlEvals %>% filter(.metric == 'roc_auc') %>% 
    count(mtry, min_n, uniq_param1)
  
  maxVal =  countEvals %>% pull(n) %>% max()
  
  rowEvals = countEvals %>% filter(n == maxVal)
  
  if(nrow(rowEvals) > 1){
    rowEvals = controlEvals %>% 
      filter(uniq_param1 %in% rowEvals$uniq_param1) %>% 
      filter(.metric == 'roc_auc') %>% 
      filter(max(.estimate) == .estimate)
    if(nrow(rowEvals) > 1){
      rowEvals = rowEvals %>% slice(1)
    }
  }
  
  rowEvals %>% select(mtry, min_n)
}

####
# Bind predictions to original data
####

bind_preds = function(params, df, fin_rfmodel){
  # getting splits and dataframes for train/test data
  # d_list output is 1) data splits 2) training data 3) test data
  d_list = splitz(df, params)
  
  #add to training data
  p_traindf = inner_join(as.data.frame(d_list[2]),
                         as.data.frame(fin_rfmodel$train_predictions),
                         by = "VIALID") %>%
    rename(type_diag = pat_type.x,
           type_pred = pat_type.y) %>%
    select(-c(cv_split,
              .pred_0,
              .pred_1,
              .row,
              .config))
  
  #add to testing
  p_testdf = inner_join(as.data.frame(d_list[3]),
                        as.data.frame(fin_rfmodel$test_predictions),
                        by = "VIALID") %>%
    rename(type_diag = pat_type.x,
           type_pred = pat_type.y) %>%
    select(-c(.row,
              .config,
              .pred_0,
              .pred_1,
              .row,
              .config))
  
  list(p_traindf, p_testdf)
}


#make function with entire process for running everything 10 times
entirety = function(seed_set, df){
  # set original parameters 
  Params_list = list(seed = seed_set,
                     prop = 0.66,
                     IDVars = c("VIALID"),
                     ngrid = 20,
                     ncv = 5,
                     ntrees = 10,
                     min_n = FALSE,
                     mtry = FALSE)
  
  #grid search to get hyperparameters
  Rf_model = get_all_models(df, params_list)
  print("Grid search complete!")
  
  #get model metrics after grid search
  Rf_mets = pluck(Rf_model,"test_metrics")
  
  #update parameters
  N_params_list = Params_list
  N_params_list$min_n = Rf_mets$min_n[1]
  N_params_list$mtry = Rf_mets$mtry[1]
  N_params_list$ngrid = FALSE
  N_params_list$ntrees = 200
  
  #get final model
  Fin_rfmod = finmod_import(df, N_params_list)
  print("Final model created!")
  
  #binds predicted outcomes and returns list of dataframes [1]train [2]test
  Data_preds = bind_preds(Params_list, df, Fin_rfmod)
  
  list("final model" = Fin_rfmod,
       "data with preds" = Data_preds)
}

####Move on to analysis####
##Trauma vs healthy
out1 = entirety(seed_set = 110, df = dat_1h)
out2=entirety(seed_set=210, df=dat_1h)
out3=entirety(seed_set=310, df=dat_1h)
out4=entirety(seed_set=410, df=dat_1h)
out5=entirety(seed_set=510, df=dat_1h)
out6=entirety(seed_set=610, df=dat_1h)
out7=entirety(seed_set=710, df=dat_1h)
out8=entirety(seed_set=810, df=dat_1h)
out9=entirety(seed_set=910, df=dat_1h)
out10=entirety(seed_set=1010, df=dat_1h)


# Extract and bind features

#train features
#make helper function
get_trainfeats = function(runout){
  as.data.frame(stack(runout[["final model"]][["train_variable_importances"]])) %>%
    slice_max(order_by = values,n=20) %>%
    mutate(varname = factor(ind)) %>%
    rename(perm_imp = values) %>% 
    mutate(rank = dense_rank(desc(perm_imp))) %>%
    select(-ind)
}

#extract features
tr_feats1 = get_trainfeats(out1)
tr_feats2 = get_trainfeats(out2)
tr_feats3 = get_trainfeats(out3)
tr_feats4 = get_trainfeats(out4)
tr_feats5 = get_trainfeats(out5)
tr_feats6 = get_trainfeats(out6)
tr_feats7 = get_trainfeats(out7)
tr_feats8 = get_trainfeats(out8)
tr_feats9 = get_trainfeats(out9)
tr_feats10 = get_trainfeats(out10)

#combine into long dataset
all_train_feats = rbind(tr_feats1, tr_feats2, tr_feats3, tr_feats4, tr_feats5,
                        tr_feats6, tr_feats7, tr_feats8, tr_feats9, tr_feats10)

#test features
#make helper function
get_testfeats = function(runout){
  as.data.frame(stack(runout[["final model"]][["test_variable_importances"]])) %>%
    slice_max(order_by = values, n = 20) %>%
    mutate(varname = factor(ind)) %>%
    rename(perm_imp = values) %>% 
    mutate(rank = dense_rank(desc(perm_imp))) %>%
    select(-ind)
}

#extract
te_feats1 = get_testfeats(out1)
te_feats2 = get_testfeats(out2)
te_feats3 = get_testfeats(out3)
te_feats4 = get_testfeats(out4)
te_feats5 = get_testfeats(out5)
te_feats6 = get_testfeats(out6)
te_feats7 = get_testfeats(out7)
te_feats8 = get_testfeats(out8)
te_feats9 = get_testfeats(out9)
te_feats10 = get_testfeats(out10)

#combine into one dataset
all_test_feats = rbind(te_feats1, te_feats2, te_feats3, te_feats4, te_feats5,
                     te_feats6, te_feats7, te_feats8, te_feats9, te_feats10)


####Boxplots####
library(ggplot2)
library(matrixStats)
library(stringr)
library(ggtext)
library(Rcpp)

#make function to join training features into wide version, make varname factor
widen_tr=function(features){
  #join features from each run
  join1 = left_join(features, 
                    tr_feats1, 
                    by = "varname") %>%
    select(-rank)
  join2 = left_join(join1,
                    tr_feats2,
                    by = "varname", 
                    suffix = c(".1",".2")) %>%
    select(-rank)
  join3 = left_join(join2, 
                    tr_feats3, 
                    by = "varname", 
                    suffix = c(".2",".3")) %>%
    select(-rank)
  join4 = left_join(join3, 
                    tr_feats4, 
                    by = "varname", 
                    suffix = c(".3",".4")) %>%
    select(-rank)
  join5 = left_join(join4, 
                    tr_feats5, 
                    by = "varname", 
                    suffix = c(".4",".5")) %>%
    select(-rank)
  join6 = left_join(join5, 
                    tr_feats6, 
                    by = "varname", 
                    suffix = c(".5",".6")) %>%
    select(-rank)
  join7 = left_join(join6, 
                    tr_feats7, 
                    by = "varname", 
                    suffix = c(".6",".7")) %>%
    select(-rank)
  join8 = left_join(join7, 
                    tr_feats8, 
                    by = "varname", 
                    suffix = c(".7",".8")) %>%
    select(-rank)
  join9 = left_join(join8, 
                    tr_feats9, 
                    by = "varname", 
                    suffix = c(".8",".9")) %>%
    select(-rank)
  train_join10 = left_join(join9,
                           tr_feats10, 
                           by = "varname", 
                           suffix = c(".9",".10")) %>%
    select(-rank)
  
  #convert varname to factor and rename df
  train_feats_wide = train_join10 %>%
    mutate(varname = factor(varname))
  
  return(train_feats_wide)
}

widen_te=function(features){
  #join features from each run
  join1 = left_join(features,
                    te_feats1, 
                    by = "varname") %>%
    select(-rank)
  join2 = left_join(join1,
                    te_feats2, 
                    by = "varname", 
                    suffix = c(".1",".2")) %>%
    select(-rank)
  join3 = left_join(join2,
                    te_feats3, 
                    by = "varname", 
                    suffix = c(".2",".3")) %>%
    select(-rank)
  join4 = left_join(join3,
                    te_feats4, 
                    by = "varname", 
                    suffix = c(".3",".4")) %>%
    select(-rank)
  join5 = left_join(join4,
                    te_feats5, 
                    by = "varname", 
                    suffix = c(".4",".5")) %>%
    select(-rank)
  join6 = left_join(join5,
                    te_feats6, 
                    by = "varname", 
                    suffix = c(".5",".6")) %>%
    select(-rank)
  join7 = left_join(join6,
                    te_feats7, 
                    by = "varname", 
                    suffix = c(".6",".7")) %>%
    select(-rank)
  join8 = left_join(join7,
                    te_feats8, 
                    by = "varname", 
                    suffix = c(".7",".8")) %>%
    select(-rank)
  join9 = left_join(join8,
                    te_feats9, 
                    by = "varname", 
                    suffix = c(".8",".9")) %>%
    select(-rank)
  test_join10 = left_join(join9,
                          te_feats10, 
                          by = "varname", 
                          suffix = c(".9",".10")) %>%
    select(-rank)
  
  #convert varname to factor and rename df
  test_feats_wide=test_join10%>%
    mutate(varname=factor(varname))
  
  return(test_feats_wide)
}


#make function for summary statistics
widestats = function(df){
  fmin = do.call(pmin, c(df[2:11], list(na.rm = TRUE)))
  fQ1 = rowQuantiles(as.matrix(df[,2:11]), probs = 0.25, na.rm = TRUE)
  fmedian = rowMedians(as.matrix(df[,2:11]), na.rm = TRUE)
  fmean = rowMeans(df[,2:11], na.rm = TRUE)
  fQ3 = rowQuantiles(as.matrix(df[,2:11]), probs = 0.75, na.rm = TRUE)
  fmax = do.call(pmax, c(df[2:11], list(na.rm = TRUE)))
  #gather sum stats
  summat = cbind(fmin, fQ1, fmedian, fmean, fQ3, fmax)
  
  return(cbind(df,summat))
}

#make function to get relative permutation importance
rel_permimp=function(wide_df){
  wide_rip = wide_df%>%
    mutate(across(perm_imp.1:perm_imp.10, 
                  \(x) (x/max(x, na.rm=T))*100, 
                  .names = "rel_{.col}"))
  
  return(wide_rip)
}

rel_stats<-function(df){
  fmin = do.call(pmin, c(df[12:21], list(na.rm = TRUE)))
  fQ1 = rowQuantiles(as.matrix(df[,12:21]), probs = 0.25, na.rm = TRUE)
  fmedian = rowMedians(as.matrix(df[,12:21]), na.rm = TRUE)
  fmean = rowMeans(df[,12:21], na.rm = TRUE)
  fQ3 = rowQuantiles(as.matrix(df[,12:21]), probs = 0.75, na.rm = TRUE)
  fmax = do.call(pmax, c(df[12:21], list(na.rm = TRUE)))
  #gather sum stats
  summat = cbind(fmin, fQ1, fmedian, fmean, fQ3, fmax)
  
  return(cbind(df,summat))
}


#Calculate number of non-NA iterations
count_na <- \(x) sum(!is.na(x))


####Features and percentages####

#get wide dataset
all_test_feats2 = all_test_feats %>%
  #filter(rank<11) %>% 
  mutate_if(is.factor, as.character) %>%
  select(-rank) %>%
  arrange(varname)

#get list of distinct features
test_feats_list = distinct(all_test_feats2,
                           across(varname))

#apply function to make into wide version
test_feats_wide = widen_te(test_feats_list)

#get relative permutation importance
test_wide_rip = rel_permimp(test_feats_wide)

#get summary stats
te_rip_stats = rel_stats(test_wide_rip)

#calculate how many iterations the variable is in
te_rip_stats$count_na <- te_rip_stats %>%
  select(starts_with("perm_imp")) %>%
  apply(., 1, count_na)

overall_f<-te_rip_stats %>% 
  filter(count_na>=5) %>% 
  select(varname)


#make graph for test data
ovplot<-ggplot(te_rip_stats %>% 
  filter(count_na>=5)) + #%>% 
  #filter(plot_prop>0 | is.na(plot_prop))) +
  labs(x="Feature", 
       y="Relative Permutation Importance",
       fill="Proportion of outcome\n with the feature") + 
  theme_bw() +
  theme(plot.title = element_markdown(hjust = 0.5)) + 
  geom_boxplot(aes(#fill=plot_prop, 
    x = reorder(varname, fmean, na.rm = TRUE),
    ymin = fmin,
    lower = fQ1,
    middle = fmedian,
    upper = fQ3,
    ymax = fmax), 
    stat = "identity") + 
  geom_point(aes(y = fmean, 
                 x = varname)) +
  coord_flip()
```

```{r RF severity}
library(tidymodels)
library(caret)
library(ranger)
library(e1071)
library(vip)

rm(list=ls()[!(grepl("dat_tidy", ls()))])

dat_2h<-dat_tidy %>% 
  filter(severity %in% c(1, 4)) %>% 
  mutate(severity = factor(case_when(severity == 1 ~ 1,
                                     TRUE ~ 0)),
         Sex = case_when(Sex == "Female" ~ 0,
                         TRUE ~ 1)) %>% 
  select(-c("pat_type", "any_covid", "died"))


# set original parameters for this analysis
params_list = list(seed = 110,
                   prop = 0.66,
                   IDVars = c("VIALID"),
                   ntrees = 10, 
                   ngrid = 20, 
                   ncv = 5,
                   min_n = FALSE,
                   mtry = FALSE)

# Function for getting training and testing models
get_all_models = function(df, params){
  
  # structuring recipe
  rec = cookbook(df)
  
  #get splits, train/test data
  # d_list output is 1) data splits 2) training data 3) test data
  d_list = splitz(df, params)
  
  #tune random forest hyperparams with grid_search from training data
  m_spec = tune_import_og(params) #specify tuning model 
  t_model = tune_folds(d_list[[2]], m_spec, rec, params) #run grid search
  t_metrics = t_model %>% collect_metrics() #pull metrics from grid search
  t_metrics$trees = params$ntrees #pull ntrees from params
  
  #select model with highest roc_auc
  f_params = select_best(t_model, "roc_auc")
  f_model = finalize_mod(f_params, params)
  
  # obtain the model output for the test set
  v_model = valid_mod(d_list[[1]], rec, f_model)
  v_metrics = v_model %>% collect_metrics() %>% 
    cbind(f_params)
  v_metrics$trees = params$ntrees
  
  # capture all model output in a list and return
  list('train_metrics' = t_metrics,
       'train_notes' = t_model$.notes,
       'best_model' = f_model, 
       'test_metrics' = v_metrics)
}

#Prep data function
juicer = function(df){
  cookbook(df) %>%
    juice()
}

####
# make recipe
####
#make recipe w/ model and dataset; specify that ids aren't predictors
#rec is recipe name for other functs. (syntax rec=cookbook(args))
#prep() to update recipe with parameters that don't need data for training
cookbook = function(df){
  recipe(formula = severity ~ .,
         data = df)%>%
    update_role(c(VIALID),
                new_role = "ID") #%>%
   # prep()
}


#Split data function
splitz = function(df, params){
  
  #set seed for reproducibility
  set.seed(params$seed)
  
  #make split object
  data_split = initial_split(df,
                             prop = params$prop,
                             strata = "severity")
  #create train and test data
  train = training(data_split)
  test = testing(data_split)
  
  list(data_split, train, test)
}


#function for initial tuning model to plug into get_all_models
tune_import_og = function(params){
  
  #make tuning model
  a_model = rand_forest(
    mtry = tune(),
    trees = !!params$ntrees,
    min_n = tune() 
  ) 
  
  a_model %>%
    set_mode("classification") %>%
    set_engine("ranger", 
               seed = !!params$seed)
}                 

#new function for tuning to plug into other functions to be used after og tuning
#
#different from other tuning function because it collects permutation importance;
#   this isn't collected with initial tuning because it's computationally 
#   expensive and only necessary  for final model
tune_import_new = function(params){
  
  #make tuning model
  a_model = rand_forest(
    mtry = !!params$mtry,
    trees = !!params$ntrees,
    min_n = !!params$min_n 
  ) 
  
  
  a_model %>% 
    set_mode("classification") %>%
    set_engine("ranger", 
               importance = "permutation",
               seed = !!params$seed)
}

####
# tune model hyperparameters plus cross validation
####
tune_folds = function(df, model, rec, params){
  tunewf = workflow()%>%
    add_model(model) %>% 
    add_recipe(rec)
  
  set.seed(params$seed)
  folds = vfold_cv(df,
                   params$ncv)
  
  #if ngrid is a number use grid search to find best hyperparams
  #after tuning, change ngrid to FALSE to trigger else and cross validate hyperparams
  
  if(params$ngrid){
    set.seed(params$seed)
    tune_grid(
      tunewf,
      resamples = folds,
      grid = params$ngrid,
      metrics = metric_set(pr_auc, roc_auc)
    )
  } else{
    control = control_resamples(save_pred = TRUE)
    set.seed(params$seed)
    fit_resamples(tunewf,
                  folds,
                  metrics = metric_set(pr_auc, roc_auc),
                  control = control)
  }
  
}

#tunes model with old parameters, then updates with new ones
finalize_mod <- function(new_parms, params){
  tune_spec <- tune_import_new(params)
  
  finalize_model(
    tune_spec,
    new_parms
  )
}

#validates model 
#fits data one more time on training data, then validates on test set
#used in fin_mod_import
valid_mod <- function(data_splits, rec, fin_rf){
  final_wf <- workflow() %>%
    add_recipe(rec) %>%
    add_model(fin_rf)
  
  final_wf %>%
    last_fit(data_splits,
             metrics = metric_set(pr_auc, roc_auc))
}


#extracts variable importance after tuning model
#used in fin_mod_import
v_important = function(df, final_mod, rec, params){
  import_mod = final_mod
  
  if(pluck(params, 'ngrid')){
    import_mod %<>% 
      set_engine("ranger", 
                 importance = "permutation",
                 seed = !!params$seed)
  }
  
  import_wf = workflow() %>%
    add_model(import_mod) %>% 
    add_recipe(rec)
  
  imp_fit = import_wf %>% 
    fit(df)
  
  pluck(imp_fit, 'fit', 'fit', 'fit', 'variable.importance') %>% 
    sort(decreasing = TRUE)
}


#trains and validates  final model and extracts variable importance
#output is a list:
#  list('train_model' = t_model,
#   'train_metrics' = t_metrics, 
#   'train_predictions' = train_pred_df,
#   'train_variable_importances' = train_var_imps,
#   'test_model' = val_m,
#   'test_metrics' = val_metrics,
#   'test_predictions' = test_pred_df,
#   'test_variable_importances' = test_var_imps)

finmod_import = function(df, params){
  # structuring recipe
  rec = cookbook(df)
  
  # getting splits and dataframes for train/test data
  # d_list output is 1) data splits 2) training data 3) test data
  d_list = splitz(df,params)
  
  # set a RF model specification with designated variables from grid search
  m_spec = tune_import_new(params)
  
  # train model given a recipe and model parameters
  print("Training model...")
  t_model = tune_folds(d_list[[2]], m_spec, rec, params)
  t_metrics = t_model %>% 
    collect_metrics()
  t_metrics$trees = params$ntrees
  
  #map train predictions to patient id
  print("Mapping training predictions...")
  train_pred_df = map(t_model$.predictions,
                      function(x){
                        xRows = x$.row
                        vialid = d_list[[2]] %>% 
                          slice(xRows) %>% 
                          pull(VIALID)
                        cbind(x, "VIALID" = vialid)
                      }) %>% 
    rbindlist(idcol = "cv_split")
  
  train_var_imps = v_important(d_list[[2]], m_spec, rec, params)
  test_var_imps = v_important(d_list[[3]], m_spec, rec, params)
  
  # run the model on the test set
  rf_params = params[c('mtry', 'min_n', 'ntrees')] %>% data.frame()
  
  print("Validating model...")
  val_m = valid_mod(d_list[[1]], rec, m_spec)
  val_metrics = cbind(rf_params, val_m %>% collect_metrics())
  
  #map patient ids to test predictions
  print("Mapping test predictions...")
  test_pred_df = map_dfr(val_m$.predictions, 
                         function(x){
                           vialid = d_list[[3]] %>%  
                             pull(VIALID)
                           cbind(x, "VIALID" = vialid)
                         }) %>% 
    as_tibble()
  
  
  # capture all model output in a list and return
  list('train_model' = t_model,
       'train_metrics' = t_metrics, 
       'train_predictions' = train_pred_df,
       'train_variable_importances' = train_var_imps,
       'test_model' = val_m,
       'test_metrics' = val_metrics,
       'test_predictions' = test_pred_df,
       'test_variable_importances' = test_var_imps)
  
}


#get best parameters from the sample splits
#not used if testing whole dataset -- only for multiple samples/subsets of data
obtain_parameters <- function(L_out){
  controlEvals = map_dfr(1:length(L_out), 
                         function(x) pluck(L_out, x, 'test_metrics')) %>% 
    mutate(uniq_param1 = paste0(mtry, '-', min_n),
           uniq_param2 = paste0(mtry, '-', min_n, '-', .metric))
  
  countEvals = controlEvals %>% filter(.metric == 'roc_auc') %>% 
    count(mtry, min_n, uniq_param1)
  
  maxVal =  countEvals %>% pull(n) %>% max()
  
  rowEvals = countEvals %>% filter(n == maxVal)
  
  if(nrow(rowEvals) > 1){
    rowEvals = controlEvals %>% 
      filter(uniq_param1 %in% rowEvals$uniq_param1) %>% 
      filter(.metric == 'roc_auc') %>% 
      filter(max(.estimate) == .estimate)
    if(nrow(rowEvals) > 1){
      rowEvals = rowEvals %>% slice(1)
    }
  }
  
  rowEvals %>% select(mtry, min_n)
}

####
# Bind predictions to original data
####

bind_preds = function(params, df, fin_rfmodel){
  # getting splits and dataframes for train/test data
  # d_list output is 1) data splits 2) training data 3) test data
  d_list = splitz(df, params)
  
  #add to training data
  p_traindf = inner_join(as.data.frame(d_list[2]),
                         as.data.frame(fin_rfmodel$train_predictions),
                         by = "VIALID") %>%
    rename(type_diag = severity.x,
           type_pred = severity.y) %>%
    select(-c(cv_split,
              .pred_0,
              .pred_1,
              .row,
              .config))
  
  #add to testing
  p_testdf = inner_join(as.data.frame(d_list[3]),
                        as.data.frame(fin_rfmodel$test_predictions),
                        by = "VIALID") %>%
    rename(type_diag = severity.x,
           type_pred = severity.y) %>%
    select(-c(.row,
              .config,
              .pred_0,
              .pred_1,
              .row,
              .config))
  
  list(p_traindf, p_testdf)
}


#make function with entire process for running everything 10 times
entirety = function(seed_set, df){
  # set original parameters 
  Params_list = list(seed = seed_set,
                     prop = 0.66,
                     IDVars = c("VIALID"),
                     ngrid = 20,
                     ncv = 5,
                     ntrees = 10,
                     min_n = FALSE,
                     mtry = FALSE)
  
  #grid search to get hyperparameters
  Rf_model = get_all_models(df, params_list)
  print("Grid search complete!")
  
  #get model metrics after grid search
  Rf_mets = pluck(Rf_model,"test_metrics")
  
  #update parameters
  N_params_list = Params_list
  N_params_list$min_n = Rf_mets$min_n[1]
  N_params_list$mtry = Rf_mets$mtry[1]
  N_params_list$ngrid = FALSE
  N_params_list$ntrees = 200
  
  #get final model
  Fin_rfmod = finmod_import(df, N_params_list)
  print("Final model created!")
  
  #binds predicted outcomes and returns list of dataframes [1]train [2]test
  Data_preds = bind_preds(Params_list, df, Fin_rfmod)
  
  list("final model" = Fin_rfmod,
       "data with preds" = Data_preds)
}



####Move on to analysis####
##Trauma vs healthy
out1 = entirety(seed_set = 110, df = dat_2h)
out2 = entirety(seed_set = 210, df = dat_2h)
out3 = entirety(seed_set = 310, df = dat_2h)
out4 = entirety(seed_set = 410, df = dat_2h)
out5 = entirety(seed_set = 510, df = dat_2h)
out6 = entirety(seed_set = 610, df = dat_2h)
out7 = entirety(seed_set = 710, df = dat_2h)
out8 = entirety(seed_set = 810, df = dat_2h)
out9 = entirety(seed_set = 910, df = dat_2h)
out10 = entirety(seed_set = 1010, df = dat_2h)


# Extract and bind features

#train features
#make helper function
get_trainfeats = function(runout){
  as.data.frame(stack(runout[["final model"]][["train_variable_importances"]])) %>%
    slice_max(order_by = values,n=20) %>%
    mutate(varname = factor(ind)) %>%
    rename(perm_imp = values) %>% 
    mutate(rank = dense_rank(desc(perm_imp))) %>%
    select(-ind)
}

#extract features
tr_feats1 = get_trainfeats(out1)
tr_feats2 = get_trainfeats(out2)
tr_feats3 = get_trainfeats(out3)
tr_feats4 = get_trainfeats(out4)
tr_feats5 = get_trainfeats(out5)
tr_feats6 = get_trainfeats(out6)
tr_feats7 = get_trainfeats(out7)
tr_feats8 = get_trainfeats(out8)
tr_feats9 = get_trainfeats(out9)
tr_feats10 = get_trainfeats(out10)

#combine into long dataset
all_train_feats=rbind(tr_feats1, tr_feats2, tr_feats3, tr_feats4, tr_feats5,
                      tr_feats6, tr_feats7, tr_feats8, tr_feats9, tr_feats10)

#test features
#make helper function
get_testfeats = function(runout){
  as.data.frame(stack(runout[["final model"]][["test_variable_importances"]])) %>%
    slice_max(order_by = values, n = 20) %>%
    mutate(varname = factor(ind)) %>%
    rename(perm_imp = values) %>% 
    mutate(rank = dense_rank(desc(perm_imp))) %>%
    select(-ind)
}

#extract
te_feats1 = get_testfeats(out1)
te_feats2 = get_testfeats(out2)
te_feats3 = get_testfeats(out3)
te_feats4 = get_testfeats(out4)
te_feats5 = get_testfeats(out5)
te_feats6 = get_testfeats(out6)
te_feats7 = get_testfeats(out7)
te_feats8 = get_testfeats(out8)
te_feats9 = get_testfeats(out9)
te_feats10 = get_testfeats(out10)

#combine into one dataset
all_test_feats=rbind(te_feats1, te_feats2, te_feats3, te_feats4, te_feats5,
                     te_feats6, te_feats7, te_feats8, te_feats9, te_feats10)



####Boxplots####
library(ggplot2)
library(matrixStats)
library(stringr)
library(ggtext)
library(Rcpp)


#make function to join training features into wide version, make varname factor
widen_tr=function(features){
  #join features from each run
  join1 = left_join(features, 
                    tr_feats1, 
                    by = "varname") %>%
    select(-rank)
  join2 = left_join(join1,
                    tr_feats2,
                    by = "varname", 
                    suffix = c(".1",".2")) %>%
    select(-rank)
  join3 = left_join(join2, 
                    tr_feats3, 
                    by = "varname", 
                    suffix = c(".2",".3")) %>%
    select(-rank)
  join4 = left_join(join3, 
                    tr_feats4, 
                    by = "varname", 
                    suffix = c(".3",".4")) %>%
    select(-rank)
  join5 = left_join(join4, 
                    tr_feats5, 
                    by = "varname", 
                    suffix = c(".4",".5")) %>%
    select(-rank)
  join6 = left_join(join5, 
                    tr_feats6, 
                    by = "varname", 
                    suffix = c(".5",".6")) %>%
    select(-rank)
  join7 = left_join(join6, 
                    tr_feats7, 
                    by = "varname", 
                    suffix = c(".6",".7")) %>%
    select(-rank)
  join8 = left_join(join7, 
                    tr_feats8, 
                    by = "varname", 
                    suffix = c(".7",".8")) %>%
    select(-rank)
  join9 = left_join(join8, 
                    tr_feats9, 
                    by = "varname", 
                    suffix = c(".8",".9")) %>%
    select(-rank)
  train_join10 = left_join(join9,
                           tr_feats10, 
                           by = "varname", 
                           suffix = c(".9",".10")) %>%
    select(-rank)
  
  #convert varname to factor and rename df
  train_feats_wide = train_join10 %>%
    mutate(varname = factor(varname))
  
  return(train_feats_wide)
}

widen_te=function(features){
  #join features from each run
  join1 = left_join(features,
                    te_feats1, 
                    by = "varname") %>%
    select(-rank)
  join2 = left_join(join1,
                    te_feats2, 
                    by = "varname", 
                    suffix = c(".1",".2")) %>%
    select(-rank)
  join3 = left_join(join2,
                    te_feats3, 
                    by = "varname", 
                    suffix = c(".2",".3")) %>%
    select(-rank)
  join4 = left_join(join3,
                    te_feats4, 
                    by = "varname", 
                    suffix = c(".3",".4")) %>%
    select(-rank)
  join5 = left_join(join4,
                    te_feats5, 
                    by = "varname", 
                    suffix = c(".4",".5")) %>%
    select(-rank)
  join6 = left_join(join5,
                    te_feats6, 
                    by = "varname", 
                    suffix = c(".5",".6")) %>%
    select(-rank)
  join7 = left_join(join6,
                    te_feats7, 
                    by = "varname", 
                    suffix = c(".6",".7")) %>%
    select(-rank)
  join8 = left_join(join7,
                    te_feats8, 
                    by = "varname", 
                    suffix = c(".7",".8")) %>%
    select(-rank)
  join9 = left_join(join8,
                    te_feats9, 
                    by = "varname", 
                    suffix = c(".8",".9")) %>%
    select(-rank)
  test_join10 = left_join(join9,
                          te_feats10, 
                          by = "varname", 
                          suffix = c(".9",".10")) %>%
    select(-rank)
  
  #convert varname to factor and rename df
  test_feats_wide = test_join10%>%
    mutate(varname = factor(varname))
  
  return(test_feats_wide)
}


#make function for summary statistics
widestats = function(df){
  fmin = do.call(pmin, c(df[2:11], list(na.rm = TRUE)))
  fQ1 = rowQuantiles(as.matrix(df[,2:11]), probs = 0.25, na.rm = TRUE)
  fmedian = rowMedians(as.matrix(df[,2:11]), na.rm = TRUE)
  fmean = rowMeans(df[,2:11], na.rm = TRUE)
  fQ3 = rowQuantiles(as.matrix(df[,2:11]), probs = 0.75, na.rm = TRUE)
  fmax = do.call(pmax, c(df[2:11], list(na.rm = TRUE)))
  #gather sum stats
  summat = cbind(fmin, fQ1, fmedian, fmean, fQ3, fmax)
  
  return(cbind(df, summat))
}

#make function to get relative permutation importance
rel_permimp = function(wide_df){
  wide_rip = wide_df %>%
    mutate(across(perm_imp.1:perm_imp.10, 
                  \(x) (x/max(x, na.rm = T))*100, 
                  .names = "rel_{.col}"))
  
  return(wide_rip)
}

rel_stats<-function(df){
  fmin = do.call(pmin, c(df[12:21], list(na.rm = TRUE)))
  fQ1 = rowQuantiles(as.matrix(df[,12:21]), probs = 0.25, na.rm = TRUE)
  fmedian = rowMedians(as.matrix(df[,12:21]), na.rm = TRUE)
  fmean = rowMeans(df[,12:21], na.rm = TRUE)
  fQ3 = rowQuantiles(as.matrix(df[,12:21]), probs = 0.75, na.rm = TRUE)
  fmax = do.call(pmax, c(df[12:21], list(na.rm = TRUE)))
  #gather sum stats
  summat = cbind(fmin, fQ1, fmedian, fmean, fQ3, fmax)
  
  return(cbind(df, summat))
}


#Calculate number of non-NA iterations
count_na <- \(x) sum(!is.na(x))


####Features and percentages####

#get wide dataset
all_test_feats2 = all_test_feats %>%
  filter(rank<11) %>% 
  mutate_if(is.factor, as.character) %>%
  select(-rank) %>%
  arrange(varname)

#get list of distinct features
test_feats_list = distinct(all_test_feats2,
                           across(varname))

#apply function to make into wide version
test_feats_wide = widen_te(test_feats_list)

#get relative permutation importance
test_wide_rip = rel_permimp(test_feats_wide)

#get summary stats
te_rip_stats = rel_stats(test_wide_rip)

#calculate how many iterations the variable is in
te_rip_stats$count_na <- te_rip_stats %>%
  select(starts_with("perm_imp")) %>%
  apply(., 1, count_na)

overall_f<-te_rip_stats %>% 
  filter(count_na >= 5) %>% 
  select(varname)

#make graph for test data
ovplot<-ggplot(te_rip_stats) + #%>% 
  #filter(plot_prop>0 | is.na(plot_prop))) +
  labs(x="Feature", 
       y="Relative Permutation Importance",
       fill="Proportion of outcome\n with the feature") + 
  theme_bw() +
  theme(plot.title = element_markdown(hjust = 0.5)) + 
  geom_boxplot(aes(x=reorder(varname,fmean, na.rm = TRUE),
                   ymin = fmin,
                   lower = fQ1,
                   middle = fmedian,
                   upper = fQ3,
                   ymax = fmax), 
               stat = "identity") + 
  geom_point(aes(y = fmean, 
                 x = varname)) +
  coord_flip()

```

```{r RF death}
library(tidymodels)
library(caret)
library(ranger)
library(e1071)
library(vip)

rm(list=ls()[!(grepl("dat_tidy", ls()))])

dat_3h<-dat_tidy %>% 
  filter(died != "healthy") %>% 
  mutate(died = factor(case_when(died == "Died" ~ 1,
                           TRUE ~ 0)),
         Sex = case_when(Sex == "Female" ~ 0,
                         TRUE ~ 1)) %>% 
  select(-c("pat_type", "any_covid", "severity")) %>% 
    na.omit()


# set original parameters for this analysis
params_list = list(seed = 110,
                   prop = 0.66,
                   IDVars = c("VIALID"),
                   ntrees = 10, 
                   ngrid = 20, 
                   ncv = 5,
                   min_n = FALSE,
                   mtry = FALSE)

# Function for getting training and testing models
get_all_models = function(df, params){
  
  # structuring recipe
  rec = cookbook(df)
  
  #get splits, train/test data
  # d_list output is 1) data splits 2) training data 3) test data
  d_list = splitz(df, params)
  
  #tune random forest hyperparams with grid_search from training data
  m_spec = tune_import_og(params) #specify tuning model 
  t_model = tune_folds(d_list[[2]], m_spec, rec, params) #run grid search
  t_metrics = t_model %>% collect_metrics() #pull metrics from grid search
  t_metrics$trees = params$ntrees #pull ntrees from params
  
  #select model with highest roc_auc
  f_params=select_best(t_model, "roc_auc")
  f_model=finalize_mod(f_params, params)
  
  # obtain the model output for the test set
  v_model = valid_mod(d_list[[1]], rec, f_model)
  v_metrics = v_model %>% collect_metrics() %>% 
    cbind(f_params)
  v_metrics$trees = params$ntrees
  
  # capture all model output in a list and return
  list('train_metrics' = t_metrics,
       'train_notes' = t_model$.notes,
       'best_model' = f_model, 
       'test_metrics' = v_metrics)
}

#Prep data function
juicer = function(df){
  cookbook(df) %>%
    juice()
}

####
# make recipe
####
#make recipe w/ model and dataset; specify that ids aren't predictors
#rec is recipe name for other functs. (syntax rec=cookbook(args))
#prep() to update recipe with parameters that don't need data for training
cookbook=function(df){
  recipe(formula = died ~ .,
         data = df)%>%
    update_role(c(VIALID),
                new_role = "ID") #%>%
    #prep()
}

#Split data function
splitz = function(df, params){
  
  #set seed for reproducibility
  set.seed(params$seed)
  
  #make split object
  data_split = initial_split(df,
                             prop = params$prop,
                             strata = "died")
  #create train and test data
  train = training(data_split)
  test = testing(data_split)
  
  list(data_split, train, test)
}


#function for initial tuning model to plug into get_all_models
tune_import_og = function(params){
  
  #make tuning model
  a_model = rand_forest(
    mtry = tune(),
    trees = !!params$ntrees,
    min_n = tune() 
  ) 
  
  a_model %>%
    set_mode("classification") %>%
    set_engine("ranger", 
               seed = !!params$seed)
}                 

#new function for tuning to plug into other functions to be used after og tuning
#
#different from other tuning function because it collects permutation importance;
#   this isn't collected with initial tuning because it's computationally 
#   expensive and only necessary  for final model
tune_import_new = function(params){
  
  #make tuning model
  a_model = rand_forest(
    mtry = !!params$mtry,
    trees = !!params$ntrees,
    min_n = !!params$min_n 
  ) 
  
  
  a_model %>% 
    set_mode("classification") %>%
    set_engine("ranger", 
               importance = "permutation",
               seed = !!params$seed)
}

####
# tune model hyperparameters plus cross validation
####
tune_folds = function(df, model, rec, params){
  tunewf = workflow() %>%
    add_recipe(rec) %>%
    add_model(model)
  
  set.seed(params$seed)
  folds = vfold_cv(df,
                   params$ncv)
  
  #if ngrid is a number use grid search to find best hyperparams
  #after tuning, change ngrid to FALSE to trigger else and cross validate hyperparams
  
  if(params$ngrid){
    set.seed(params$seed)
    tune_grid(
      tunewf,
      resamples = folds,
      grid = params$ngrid,
      metrics = metric_set(pr_auc, roc_auc)
    )
  } else{
    control = control_resamples(save_pred = TRUE)
    set.seed(params$seed)
    fit_resamples(tunewf,
                  folds,
                  metrics = metric_set(pr_auc, roc_auc),
                  control = control)
  }
  
}

#tunes model with old parameters, then updates with new ones
finalize_mod <- function(new_parms, params){
  tune_spec <- tune_import_new(params)
  
  finalize_model(
    tune_spec,
    new_parms
  )
}

#validates model 
#fits data one more time on training data, then validates on test set
#used in fin_mod_import
valid_mod <- function(data_splits, rec, fin_rf){
  final_wf <- workflow() %>%
    add_recipe(rec) %>%
    add_model(fin_rf)
  
  final_wf %>%
    last_fit(data_splits,
             metrics = metric_set(pr_auc, roc_auc))
}


#extracts variable importance after tuning model
#used in fin_mod_import
v_important = function(df, final_mod, rec, params){
  import_mod = final_mod
  
  if(pluck(params, 'ngrid')){
    import_mod %<>% 
      set_engine("ranger", 
                 importance = "permutation",
                 seed = !!params$seed)
  }
  
  import_wf = workflow() %>%
    add_recipe(rec) %>% 
    add_model(import_mod)
  
  imp_fit = import_wf %>% 
    fit(df)
  
  pluck(imp_fit, 'fit', 'fit', 'fit', 'variable.importance') %>% 
    sort(decreasing = TRUE)
}


#trains and validates  final model and extracts variable importance
#output is a list:
#  list('train_model' = t_model,
#   'train_metrics' = t_metrics, 
#   'train_predictions' = train_pred_df,
#   'train_variable_importances' = train_var_imps,
#   'test_model' = val_m,
#   'test_metrics' = val_metrics,
#   'test_predictions' = test_pred_df,
#   'test_variable_importances' = test_var_imps)

finmod_import = function(df, params){
  # structuring recipe
  rec = cookbook(df)
  
  # getting splits and dataframes for train/test data
  # d_list output is 1) data splits 2) training data 3) test data
  d_list = splitz(df,params)
  
  # set a RF model specification with designated variables from grid search
  m_spec = tune_import_new(params)
  
  # train model given a recipe and model parameters
  print("Training model...")
  t_model = tune_folds(d_list[[2]], m_spec, rec, params)
  t_metrics = t_model %>% 
    collect_metrics()
  t_metrics$trees = params$ntrees
  
  #map train predictions to patient id
  print("Mapping training predictions...")
  train_pred_df = map(t_model$.predictions,
                      function(x){
                        xRows = x$.row
                        vialid = d_list[[2]] %>% 
                          slice(xRows) %>% 
                          pull(VIALID)
                        cbind(x, "VIALID" = vialid)
                      }) %>% 
    rbindlist(idcol = "cv_split")
  
  train_var_imps = v_important(d_list[[2]], m_spec, rec, params)
  test_var_imps = v_important(d_list[[3]], m_spec, rec, params)
  
  # run the model on the test set
  rf_params = params[c('mtry', 'min_n', 'ntrees')] %>% data.frame()
  
  print("Validating model...")
  val_m = valid_mod(d_list[[1]], rec, m_spec)
  val_metrics = cbind(rf_params, val_m %>% collect_metrics())
  
  #map patient ids to test predictions
  print("Mapping test predictions...")
  test_pred_df = map_dfr(val_m$.predictions, 
                         function(x){
                           vialid = d_list[[3]] %>%  
                             pull(VIALID)
                           cbind(x, "VIALID" = vialid)
                         }) %>% 
    as_tibble()
  
  
  # capture all model output in a list and return
  list('train_model' = t_model,
       'train_metrics' = t_metrics, 
       'train_predictions' = train_pred_df,
       'train_variable_importances' = train_var_imps,
       'test_model' = val_m,
       'test_metrics' = val_metrics,
       'test_predictions' = test_pred_df,
       'test_variable_importances' = test_var_imps)
  
}


#get best parameters from the sample splits
#not used if testing whole dataset -- only for multiple samples/subsets of data
obtain_parameters <- function(L_out){
  controlEvals = map_dfr(1:length(L_out), 
                         function(x) pluck(L_out, x, 'test_metrics')) %>% 
    mutate(uniq_param1 = paste0(mtry, '-', min_n),
           uniq_param2 = paste0(mtry, '-', min_n, '-', .metric))
  
  countEvals = controlEvals %>% filter(.metric == 'roc_auc') %>% 
    count(mtry, min_n, uniq_param1)
  
  maxVal =  countEvals %>% pull(n) %>% max()
  
  rowEvals = countEvals %>% filter(n == maxVal)
  
  if(nrow(rowEvals) > 1){
    rowEvals = controlEvals %>% 
      filter(uniq_param1 %in% rowEvals$uniq_param1) %>% 
      filter(.metric == 'roc_auc') %>% 
      filter(max(.estimate) == .estimate)
    if(nrow(rowEvals) > 1){
      rowEvals = rowEvals %>% slice(1)
    }
  }
  
  rowEvals %>% select(mtry, min_n)
}



####
# Bind predictions to original data
####

bind_preds = function(params, df, fin_rfmodel){
  # getting splits and dataframes for train/test data
  # d_list output is 1) data splits 2) training data 3) test data
  d_list = splitz(df, params)
  
  #add to training data
  p_traindf = inner_join(as.data.frame(d_list[2]),
                         as.data.frame(fin_rfmodel$train_predictions),
                         by = "VIALID") %>%
    rename(type_diag = died.x,
           type_pred = died.y) %>%
    select(-c(cv_split,
              .pred_0,
              .pred_1,
              .row,
              .config))
  
  #add to testing
  p_testdf = inner_join(as.data.frame(d_list[3]),
                        as.data.frame(fin_rfmodel$test_predictions),
                        by = "VIALID") %>%
    rename(type_diag = died.x,
           type_pred = died.y) %>%
    select(-c(.row,
              .config,
              .pred_0,
              .pred_1,
              .row,
              .config))
  
  list(p_traindf, p_testdf)
}


#make function with entire process for running everything 10 times
entirety = function(seed_set, df){
  # set original parameters 
  Params_list = list(seed = seed_set,
                     prop = 0.66,
                     IDVars = c("VIALID"),
                     ngrid = 20,
                     ncv = 5,
                     ntrees = 10,
                     min_n = FALSE,
                     mtry = FALSE)
  
  #grid search to get hyperparameters
  Rf_model = get_all_models(df, params_list)
  print("Grid search complete!")
  
  #get model metrics after grid search
  Rf_mets = pluck(Rf_model,"test_metrics")
  
  #update parameters
  N_params_list = Params_list
  N_params_list$min_n = Rf_mets$min_n[1]
  N_params_list$mtry = Rf_mets$mtry[1]
  N_params_list$ngrid = FALSE
  N_params_list$ntrees = 200
  
  #get final model
  Fin_rfmod = finmod_import(df, N_params_list)
  print("Final model created!")
  
  #binds predicted outcomes and returns list of dataframes [1]train [2]test
  Data_preds = bind_preds(Params_list, df, Fin_rfmod)
  
  list("final model" = Fin_rfmod,
       "data with preds" = Data_preds)
}

####Move on to analysis####
##lived vs died
out1 = entirety(seed_set = 110, df = dat_3h)
out2 = entirety(seed_set = 210, df = dat_3h)
out3 = entirety(seed_set = 310, df = dat_3h)
out4 = entirety(seed_set = 410, df = dat_3h)
out5 = entirety(seed_set = 510, df = dat_3h)
out6 = entirety(seed_set = 610, df = dat_3h)
out7 = entirety(seed_set = 710, df = dat_3h)
out8 = entirety(seed_set = 810, df = dat_3h)
out9 = entirety(seed_set = 910, df = dat_3h)
out10 = entirety(seed_set = 1010, df = dat_3h)


# Extract and bind features

#train features
#make helper function
get_trainfeats = function(runout){
  as.data.frame(stack(runout[["final model"]][["train_variable_importances"]])) %>%
    slice_max(order_by = values,n=20) %>%
    mutate(varname = factor(ind)) %>%
    rename(perm_imp = values) %>% 
    mutate(rank = dense_rank(desc(perm_imp))) %>%
    select(-ind)
}

#extract features
tr_feats1 = get_trainfeats(out1)
tr_feats2 = get_trainfeats(out2)
tr_feats3 = get_trainfeats(out3)
tr_feats4 = get_trainfeats(out4)
tr_feats5 = get_trainfeats(out5)
tr_feats6 = get_trainfeats(out6)
tr_feats7 = get_trainfeats(out7)
tr_feats8 = get_trainfeats(out8)
tr_feats9 = get_trainfeats(out9)
tr_feats10 = get_trainfeats(out10)

#combine into long dataset
all_train_feats=rbind(tr_feats1, tr_feats2, tr_feats3, tr_feats4, tr_feats5,
                      tr_feats6, tr_feats7, tr_feats8, tr_feats9, tr_feats10)

#test features
#make helper function
get_testfeats = function(runout){
  as.data.frame(stack(runout[["final model"]][["test_variable_importances"]])) %>%
    slice_max(order_by = values, n = 20) %>%
    mutate(varname = factor(ind)) %>%
    rename(perm_imp = values) %>% 
    mutate(rank = dense_rank(desc(perm_imp))) %>%
    select(-ind)
}

#extract
te_feats1 = get_testfeats(out1)
te_feats2 = get_testfeats(out2)
te_feats3 = get_testfeats(out3)
te_feats4 = get_testfeats(out4)
te_feats5 = get_testfeats(out5)
te_feats6 = get_testfeats(out6)
te_feats7 = get_testfeats(out7)
te_feats8 = get_testfeats(out8)
te_feats9 = get_testfeats(out9)
te_feats10 = get_testfeats(out10)

#combine into one dataset
all_test_feats=rbind(te_feats1, te_feats2, te_feats3, te_feats4, te_feats5,
                     te_feats6, te_feats7, te_feats8, te_feats9, te_feats10)



####Boxplots####
library(ggplot2)
library(matrixStats)
library(stringr)
library(ggtext)
library(Rcpp)

#make function to join training features into wide version, make varname factor
widen_tr=function(features){
  #join features from each run
  join1 = left_join(features, 
                    tr_feats1, 
                    by = "varname") %>%
    select(-rank)
  join2 = left_join(join1,
                    tr_feats2,
                    by = "varname", 
                    suffix = c(".1",".2")) %>%
    select(-rank)
  join3 = left_join(join2, 
                    tr_feats3, 
                    by = "varname", 
                    suffix = c(".2",".3")) %>%
    select(-rank)
  join4 = left_join(join3, 
                    tr_feats4, 
                    by = "varname", 
                    suffix = c(".3",".4")) %>%
    select(-rank)
  join5 = left_join(join4, 
                    tr_feats5, 
                    by = "varname", 
                    suffix = c(".4",".5")) %>%
    select(-rank)
  join6 = left_join(join5, 
                    tr_feats6, 
                    by = "varname", 
                    suffix = c(".5",".6")) %>%
    select(-rank)
  join7 = left_join(join6, 
                    tr_feats7, 
                    by = "varname", 
                    suffix = c(".6",".7")) %>%
    select(-rank)
  join8 = left_join(join7, 
                    tr_feats8, 
                    by = "varname", 
                    suffix = c(".7",".8")) %>%
    select(-rank)
  join9 = left_join(join8, 
                    tr_feats9, 
                    by = "varname", 
                    suffix = c(".8",".9")) %>%
    select(-rank)
  train_join10 = left_join(join9,
                           tr_feats10, 
                           by = "varname", 
                           suffix = c(".9",".10")) %>%
    select(-rank)
  
  #convert varname to factor and rename df
  train_feats_wide = train_join10 %>%
    mutate(varname = factor(varname))
  
  return(train_feats_wide)
}

widen_te=function(features){
  #join features from each run
  join1 = left_join(features,
                    te_feats1, 
                    by = "varname") %>%
    select(-rank)
  join2 = left_join(join1,
                    te_feats2, 
                    by = "varname", 
                    suffix = c(".1",".2")) %>%
    select(-rank)
  join3 = left_join(join2,
                    te_feats3, 
                    by = "varname", 
                    suffix = c(".2",".3")) %>%
    select(-rank)
  join4 = left_join(join3,
                    te_feats4, 
                    by = "varname", 
                    suffix = c(".3",".4")) %>%
    select(-rank)
  join5 = left_join(join4,
                    te_feats5, 
                    by = "varname", 
                    suffix = c(".4",".5")) %>%
    select(-rank)
  join6 = left_join(join5,
                    te_feats6, 
                    by = "varname", 
                    suffix = c(".5",".6")) %>%
    select(-rank)
  join7 = left_join(join6,
                    te_feats7, 
                    by = "varname", 
                    suffix = c(".6",".7")) %>%
    select(-rank)
  join8 = left_join(join7,
                    te_feats8, 
                    by = "varname", 
                    suffix = c(".7",".8")) %>%
    select(-rank)
  join9 = left_join(join8,
                    te_feats9, 
                    by = "varname", 
                    suffix = c(".8",".9")) %>%
    select(-rank)
  test_join10 = left_join(join9,
                          te_feats10, 
                          by = "varname", 
                          suffix = c(".9",".10")) %>%
    select(-rank)
  
  #convert varname to factor and rename df
  test_feats_wide = test_join10 %>%
    mutate(varname = factor(varname))
  
  return(test_feats_wide)
}


#make function for summary statistics
widestats = function(df){
  fmin = do.call(pmin, c(df[2:11], list(na.rm = TRUE)))
  fQ1 = rowQuantiles(as.matrix(df[,2:11]), probs = 0.25, na.rm = TRUE)
  fmedian = rowMedians(as.matrix(df[,2:11]), na.rm = TRUE)
  fmean = rowMeans(df[,2:11], na.rm = TRUE)
  fQ3 = rowQuantiles(as.matrix(df[,2:11]), probs = 0.75, na.rm = TRUE)
  fmax = do.call(pmax, c(df[2:11], list(na.rm = TRUE)))
  #gather sum stats
  summat = cbind(fmin, fQ1, fmedian, fmean, fQ3, fmax)
  
  return(cbind(df,summat))
}

#make function to get relative permutation importance
rel_permimp = function(wide_df){
  wide_rip = wide_df%>%
    mutate(across(perm_imp.1:perm_imp.10, 
                  \(x) (x/max(x, na.rm = T))*100, 
                  .names = "rel_{.col}"))
  
  return(wide_rip)
}

rel_stats<-function(df){
  fmin = do.call(pmin, c(df[12:21], list(na.rm = TRUE)))
  fQ1 = rowQuantiles(as.matrix(df[,12:21]), probs = 0.25, na.rm = TRUE)
  fmedian = rowMedians(as.matrix(df[,12:21]), na.rm = TRUE)
  fmean = rowMeans(df[,12:21], na.rm = TRUE)
  fQ3 = rowQuantiles(as.matrix(df[,12:21]), probs = 0.75, na.rm = TRUE)
  fmax = do.call(pmax, c(df[12:21], list(na.rm = TRUE)))
  #gather sum stats
  summat = cbind(fmin, fQ1, fmedian, fmean, fQ3, fmax)
  
  return(cbind(df,summat))
}


#Calculate number of non-NA iterations
count_na <- \(x) sum(!is.na(x))

####Features and percentages####

#get wide dataset
all_test_feats2 = all_test_feats %>%
#  filter(rank<11) %>% 
  mutate_if(is.factor, as.character) %>%
  select(-rank) %>%
  arrange(varname)

#get list of distinct features
test_feats_list = distinct(all_test_feats2,
                           across(varname))

#apply function to make into wide version
test_feats_wide = widen_te(test_feats_list)

#get relative permutation importance
test_wide_rip = rel_permimp(test_feats_wide)

#get summary stats
te_rip_stats = rel_stats(test_wide_rip)

#calculate how many iterations the variable is in
te_rip_stats$count_na <- te_rip_stats %>%
  select(starts_with("perm_imp")) %>%
  apply(., 1, count_na)

overall_f<-te_rip_stats %>% 
  filter(count_na >= 5) %>% 
  select(varname)

#make graph for test data
ovplot<-ggplot(te_rip_stats %>% filter(count_na >= 5)) + #%>% 
  #filter(plot_prop>0 | is.na(plot_prop))) +
  labs(x="Feature", 
       y="Relative Permutation Importance",
       fill="Proportion of outcome\n with the feature") + 
  theme_bw() +
  theme(plot.title = element_markdown(hjust = 0.5)) + 
  geom_boxplot(aes(x = reorder(varname, fmean, na.rm = TRUE),
                   ymin = fmin,
                   lower = fQ1,
                   middle = fmedian,
                   upper = fQ3,
                   ymax = fmax), 
               stat = "identity") + 
  geom_point(aes(y = fmean, 
                 x = varname)) +
  coord_flip()
```

